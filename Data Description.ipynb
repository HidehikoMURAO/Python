{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np               \n",
    "import matplotlib.pyplot as plt  \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主成分分析を行うには scikit-learn パッケージを使用して，sklearn.decomposition の PCA でインスタンスを生成します．\n",
    "\n",
    "以下の例では，Davis データを用いて主成分分析を行っています．\n",
    "\n",
    "Davisデータ（Davis.csv）はJupyter Notebookの保存されているディレクトリと同じディレクトリに保存されているものとします．\n",
    "\n",
    "Davisデータの読み込みには pandas パッケージの pd.read_csv を使用します．\n",
    "\n",
    "データ配列の第1, 2列の各行がデータ点${\\bf{x_{i}}} = ( w_{i}, h_{i} )$に対応しています（$x_{i}$は$i$番目の人の体重[kg]，$h_{i}$は身長[cm]に対応）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA         # Use sklearn's PCA．\n",
    "dat = pd.read_csv('data/Davis.csv').values    # Read data (use pandas)\n",
    "# Convert the unit of height to [m] and calculate the value of logarithm.\n",
    "logdat = np.log(np.c_[dat[:,1],dat[:,2]/100].astype('float'))\n",
    "# Plot data\n",
    "plt.plot(logdat[:,0], logdat[:,1], '.'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA of data\n",
    "pca = PCA()    \n",
    "pca.fit(logdat) \n",
    "pca.components_       # Principal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data at index 11 are removed as outliers.\n",
    "clean_logdat = np.delete(logdat, 11, axis=0)\n",
    "\n",
    "# Principal component analysis of data from which outliers have been removed\n",
    "pca = PCA()    \n",
    "pca.fit(clean_logdat) \n",
    "pca.components_       # Principal component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 因子分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston  # BostonHousing を使う\n",
    "BostonHousing = load_boston()             # データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.preprocessing import scale\n",
    "X = scale(BostonHousing.data)  # データのスケーリング(相関行列に因子分解を適用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape                        # データ行列のサイズ：13次元506サンプル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = FactorAnalysis(n_components=3)   # 因子数3で推定\n",
    "rX = fa.fit_transform(X)              # 因子スコア"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa.components_                        # 因子負荷行列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa.components_.shape                  # サイズは(因子数, 次元)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因子負荷行列の要素：絶対値の大きさでソート．\n",
    "BostonHousing.feature_names[np.argsort(np.abs(fa.components_[0,]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多次元尺度構成法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c04aadeb31ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultivariate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfactor_rotation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrotate_factors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultivariate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfactor_rotation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrotate_factors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'varimax'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# バリマックス回転基準\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fa' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt  \n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA    # Use sklearn's PCA．\n",
    "from sklearn.datasets import load_boston  # BostonHousing を使う\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.multivariate.factor_rotation import rotate_factors\n",
    "L, T = sm.multivariate.factor_rotation.rotate_factors(fa.components_.T,'varimax')  # バリマックス回転基準\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics.pairwise import euclidean_distances \n",
    "n = 10                              # データ数\n",
    "k = 2                               # データの次元\n",
    "V = np.random.rand(n,k)             # 真の配置\n",
    "d = euclidean_distances(V)          # 距離行列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計量的MDS(次元2): 10個の初期値で計算し最適な解を採用\n",
    "md = MDS(n_components=2, metric=True, dissimilarity='precomputed', n_init=10, max_iter=3000)\n",
    "md.fit(d)\n",
    "rV2 = md.embedding_                 # 再構成された2次元点配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計量的MDS(次元1) \n",
    "md.set_params(n_components=1)          \n",
    "md.fit(d)        \n",
    "rV1 = md.embedding_                 # 再構成された1次元点配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 元データのプロット\n",
    "plt.scatter(V[:,0],V[:,1],marker='.'); \n",
    "for i,(x,y) in enumerate(zip(V[:,0],V[:,1])): # 点番号も表示\n",
    "    plt.annotate(str(i),(x,y),fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計量的MDSで再構成した点のプロット\n",
    "plt.scatter(rV2[:,0],rV2[:,1],marker='.');    # 点番号も表示\n",
    "for i,(x,y) in enumerate(zip(rV2[:,0],rV2[:,1])):\n",
    "    plt.annotate(str(i),(x,y),fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "data = pd.read_csv('data/voting.csv').values\n",
    "#  S:非類似度行列(投票行動)，pidx: 所属する党(0/1)\n",
    "S=data[:,:15]; pidx=data[:,15]  \n",
    "col=['red','blue']; mk = ['x','o'] # 所属する党を区別するマーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 非計量的MDS\n",
    "nmd = MDS(n_components=2, metric=False, dissimilarity='precomputed',  n_init=20,max_iter=3000)\n",
    "nmd.fit(S)          # フィッティング\n",
    "px = nmd.embedding_[:,0]; py = nmd.embedding_[:,1]\n",
    "for i in [0,1]:     # プロット\n",
    "    plt.scatter(px[pidx==i],py[pidx==i],c=col[i],marker=mk[i],s=100)\n",
    "for i,(x,y) in enumerate(zip(px,py)):\n",
    "    plt.annotate(str(i),(x,y),fontsize=20)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計量的MDS\n",
    "nmd.set_params(metric=True)\n",
    "nmd.fit(S)          # フィッティング\n",
    "px = nmd.embedding_[:,0]; py = nmd.embedding_[:,1]\n",
    "for i in [0,1]:     # プロット\n",
    "    plt.scatter(px[pidx==i],py[pidx==i],c=col[i],marker=mk[i],s=100)\n",
    "for i,(x,y) in enumerate(zip(px,py)):\n",
    "    plt.annotate(str(i),(x,y),fontsize=20)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
